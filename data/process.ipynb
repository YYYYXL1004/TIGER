{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert raw data to 'strict' json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os\n",
    "dataset_name = \"Beauty\"\n",
    "os.makedirs(dataset_name, exist_ok=True)\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield json.dumps(eval(l))\n",
    "\n",
    "# Beauty dataset\n",
    "f = open(f\"./{dataset_name}/{dataset_name}.json\", 'w')\n",
    "for l in parse(f\"reviews_{dataset_name}_5.json.gz\"):\n",
    "  f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 198499\n",
      "First line: {\"reviewerID\": \"A1YJEY40YUW4SE\", \"asin\": \"7806397051\", \"reviewerName\": \"Andrea\", \"helpful\": [3, 4], \"reviewText\": \"Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.\", \"overall\": 1.0, \"summary\": \"Don't waste your money\", \"unixReviewTime\": 1391040000, \"reviewTime\": \"01 30, 2014\"}\n"
     ]
    }
   ],
   "source": [
    "# print the number of lines in the file and the first line\n",
    "data = open(f\"./{dataset_name}/{dataset_name}.json\", 'r')\n",
    "print(\"Number of lines:\", sum(1 for _ in data))\n",
    "data.seek(0)  # Reset file pointer to the beginning\n",
    "print(\"First line:\", data.readline().strip())\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_num: 22363\n",
      "the first five userID mapping: [('A1YJEY40YUW4SE', 1), ('A60XNB876KYML', 2), ('A3G6XNM240RMWA', 3), ('A1PQFP6SAJ6D80', 4), ('A38FVHZTNQ271F', 5)]\n",
      "item_num: 12101\n",
      "the first five itemID mapping: [('7806397051', 1), ('9759091062', 2), ('9788072216', 3), ('9790790961', 4), ('9790794231', 5)]\n",
      "user-item mapping: [(1, [6846, 7873, 4585, 1, 5406]), (2, [816, 10406, 11194, 11651, 9716, 1, 233]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204, 5863, 6609]), (4, [5522, 439, 5161, 11140, 1, 7849]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500, 11444, 11390])]\n",
      "training data: [(1, [6846, 7873, 4585]), (2, [816, 10406, 11194, 11651, 9716]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204]), (4, [5522, 439, 5161, 11140]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500])]\n",
      "validation data: [(1, [6846, 7873, 4585, 1]), (2, [816, 10406, 11194, 11651, 9716, 1]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204, 5863]), (4, [5522, 439, 5161, 11140, 1]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500, 11444])]\n",
      "testing data: [(1, [6846, 7873, 4585, 1, 5406]), (2, [816, 10406, 11194, 11651, 9716, 1, 233]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204, 5863, 6609]), (4, [5522, 439, 5161, 11140, 1, 7849]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500, 11444, 11390])]\n",
      "\n",
      "Training data shape: (22363, 3)\n",
      "the first 3 rows of training data:\n",
      "    user                           history  target\n",
      "0     1                      [6846, 7873]    4585\n",
      "1     2        [816, 10406, 11194, 11651]    9716\n",
      "2     3  [1, 6050, 7977, 5252, 4211, 243]   11204\n",
      "\n",
      "Validation data shape: (22363, 3)\n",
      "the first 3 rows of validation data:\n",
      "    user                                  history  target\n",
      "0     1                       [6846, 7873, 4585]       1\n",
      "1     2         [816, 10406, 11194, 11651, 9716]       1\n",
      "2     3  [1, 6050, 7977, 5252, 4211, 243, 11204]    5863\n",
      "\n",
      "Testing data shape: (22363, 3)\n",
      "the first 3 rows of testing data:\n",
      "    user                                        history  target\n",
      "0     1                          [6846, 7873, 4585, 1]    5406\n",
      "1     2            [816, 10406, 11194, 11651, 9716, 1]     233\n",
      "2     3  [1, 6050, 7977, 5252, 4211, 243, 11204, 5863]    6609\n",
      "Data saved to parquet files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize mapping dictionaries\n",
    "userID_mapping = {}\n",
    "itemID_mapping = {}\n",
    "\n",
    "# Open the JSON file for reading\n",
    "data = open(f\"./{dataset_name}/{dataset_name}.json\", 'r')\n",
    "\n",
    "# Initialize lists to store userID, itemID, and timestamp\n",
    "userIDs = []\n",
    "itemIDs = []\n",
    "timestamps = []\n",
    "\n",
    "# Process each line in the JSON file\n",
    "for line in data:\n",
    "    review = json.loads(line.strip())\n",
    "    userID = review['reviewerID']\n",
    "    itemID = review['asin']\n",
    "    timestamp = review['unixReviewTime']\n",
    "    \n",
    "    # Map userID to an integer starting from 1\n",
    "    if userID not in userID_mapping:\n",
    "        userID_mapping[userID] = len(userID_mapping) + 1\n",
    "    \n",
    "    # Map itemID to an integer starting from 1\n",
    "    if itemID not in itemID_mapping:\n",
    "        itemID_mapping[itemID] = len(itemID_mapping) + 1\n",
    "    \n",
    "    # Append mapped values and timestamp to lists\n",
    "    userIDs.append(userID_mapping[userID])\n",
    "    itemIDs.append(itemID_mapping[itemID])\n",
    "    timestamps.append(timestamp)\n",
    "\n",
    "# Save mapping dictionaries as .npy files\n",
    "np.save(f'./{dataset_name}/user_mapping.npy', userID_mapping)\n",
    "print(\"user_num:\", len(userID_mapping))\n",
    "print(\"the first five userID mapping:\", list(userID_mapping.items())[:5])\n",
    "np.save(f'./{dataset_name}/item_mapping.npy', itemID_mapping)\n",
    "print(\"item_num:\", len(itemID_mapping))\n",
    "print(\"the first five itemID mapping:\", list(itemID_mapping.items())[:5])\n",
    "\n",
    "# Group itemIDs by userID and sort by timestamp\n",
    "user_item_mapping = {}\n",
    "for userID, itemID, timestamp in zip(userIDs, itemIDs, timestamps):\n",
    "    if userID not in user_item_mapping:\n",
    "        user_item_mapping[userID] = []\n",
    "    user_item_mapping[userID].append((itemID, timestamp))\n",
    "\n",
    "# Sort itemIDs for each user by timestamp\n",
    "for userID in user_item_mapping:\n",
    "    user_item_mapping[userID].sort(key=lambda x: x[1])\n",
    "    user_item_mapping[userID] = [item[0] for item in user_item_mapping[userID]]\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"user-item mapping:\", list(user_item_mapping.items())[:5])\n",
    "\n",
    "# Split data into training, validation, and testing sets using leave-one-out strategy\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for userID, item_sequence in user_item_mapping.items():\n",
    "    # Assign the last item for testing, the second-to-last for validation, and the rest for training\n",
    "    train_data[userID] = item_sequence[:-2]\n",
    "    val_data[userID] = item_sequence[:-1]\n",
    "    test_data[userID] = item_sequence\n",
    "\n",
    "# Print a sample of the split data\n",
    "print(\"training data:\", list(train_data.items())[:5])\n",
    "print(\"validation data:\", list(val_data.items())[:5])\n",
    "print(\"testing data:\", list(test_data.items())[:5])\n",
    "\n",
    "# Prepare data for train, validation, and test sets\n",
    "def prepare_data(data_dict):\n",
    "    rows = []\n",
    "    for userID, item_sequence in data_dict.items():\n",
    "        history = item_sequence[:-1]\n",
    "        target = item_sequence[-1]\n",
    "        rows.append({'user': userID, 'history': history, 'target': target})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create dataframes for train, validation, and test sets\n",
    "train_df = prepare_data(train_data)\n",
    "print(\"\\nTraining data shape:\", train_df.shape)\n",
    "print(\"the first 3 rows of training data:\\n\", train_df.head(3))\n",
    "val_df = prepare_data(val_data)\n",
    "print(\"\\nValidation data shape:\", val_df.shape)\n",
    "print(\"the first 3 rows of validation data:\\n\", val_df.head(3))\n",
    "test_df = prepare_data(test_data)\n",
    "print(\"\\nTesting data shape:\", test_df.shape)\n",
    "print(\"the first 3 rows of testing data:\\n\", test_df.head(3))\n",
    "\n",
    "# Save dataframes to parquet files\n",
    "train_df.to_parquet(f'./{dataset_name}/train.parquet', index=False)\n",
    "val_df.to_parquet(f'./{dataset_name}/valid.parquet', index=False)\n",
    "test_df.to_parquet(f'./{dataset_name}/test.parquet', index=False)\n",
    "\n",
    "print(\"Data saved to parquet files.\")\n",
    "\n",
    "data.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Item Semantic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty metadata \n",
    "f = open(f\"./{dataset_name}/{dataset_name}_metadata.json\", 'w')\n",
    "for l in parse(f\"meta_{dataset_name}.json.gz\"):\n",
    "  f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemID: 1, Info: {'title': 'WAWO 15 Color Professionl Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette', 'price': 5.04, 'salesRank': {'Beauty': 10486}, 'brand': 'COKA', 'categories': [['Beauty', 'Makeup', 'Face', 'Concealers & Neutralizers']]}\n",
      "ItemID: 2, Info: {'title': 'Xtreme Brite Brightening Gel 1oz.', 'price': 19.99, 'salesRank': {'Beauty': 52254}, 'brand': 'Xtreme Brite', 'categories': [['Beauty', 'Hair Care', 'Styling Products', 'Creams, Gels & Lotions']]}\n",
      "ItemID: 3, Info: {'title': 'Prada Candy By Prada Eau De Parfum Spray 1.7 Oz For Women', 'price': 65.86, 'salesRank': {'Beauty': 78916}, 'brand': 'Prada', 'categories': [['Beauty', 'Fragrance', \"Women's\", 'Eau de Parfum']]}\n",
      "ItemID: 4, Info: {'title': 'Versace Bright Crystal Eau de Toilette Spray for Women, 3 Ounce', 'price': 52.33, 'salesRank': {'Beauty': 764}, 'brand': 'Versace', 'categories': [['Beauty', 'Fragrance', \"Women's\", 'Eau de Toilette']]}\n",
      "ItemID: 5, Info: {'title': 'Stella McCartney Stella', 'price': None, 'salesRank': {'Beauty': 142503}, 'brand': None, 'categories': [['Beauty', 'Fragrance', \"Women's\", 'Eau de Parfum']]}\n"
     ]
    }
   ],
   "source": [
    "# Open the metadata file for reading\n",
    "with open(f\"./{dataset_name}/{dataset_name}_metadata.json\", 'r') as metadata_file:\n",
    "    # Create a reverse mapping from itemID to asin\n",
    "    reverse_itemID_mapping = {v: k for k, v in itemID_mapping.items()}\n",
    "    \n",
    "    # Initialize a dictionary to store the extracted information\n",
    "    item_info = {}\n",
    "    \n",
    "    # Process each line in the metadata file\n",
    "    for line in metadata_file:\n",
    "        metadata = json.loads(line.strip())\n",
    "        asin = metadata.get('asin')\n",
    "        \n",
    "        # Check if the asin exists in the reverse mapping\n",
    "        if asin in reverse_itemID_mapping.values():\n",
    "            itemID = itemID_mapping[asin]\n",
    "            item_info[itemID] = {\n",
    "                'title': metadata.get('title') if metadata.get('title') else None,\n",
    "                'price': metadata.get('price') if metadata.get('price') else None,\n",
    "                'salesRank': metadata.get('salesRank') if metadata.get('salesRank') else None,\n",
    "                'brand': metadata.get('brand') if metadata.get('brand') else None,\n",
    "                'categories': metadata.get('categories') if metadata.get('categories') else None,\n",
    "            }\n",
    "\n",
    "# Print the information for the first 5 items\n",
    "for itemID, info in list(item_info.items())[:5]:\n",
    "    print(f\"ItemID: {itemID}, Info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the SentenceTransformer model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#  modelscope download --model sentence-transformers/sentence-t5-base  --local_dir ./dir\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../sentence-t5-base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Prepare data for embedding\u001b[39;00m\n\u001b[1;32m     14\u001b[0m item_embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TIGER/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3,4'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device('cuda:1')\n",
    "# Initialize the SentenceTransformer model\n",
    "#  modelscope download --model sentence-transformers/sentence-t5-base  --local_dir ./dir\n",
    "model = SentenceTransformer('../sentence-t5-base').to('cpu')\n",
    "\n",
    "\n",
    "# Prepare data for embedding\n",
    "item_embeddings = []\n",
    "for itemID, info in item_info.items():\n",
    "    # Combine relevant fields into a single text for embedding\n",
    "    semantics = f\"'title':{info.get('title', '')}\\n 'price':{info.get('price', '')}\\n 'salesRank':{info.get('salesRank', '')}\\n 'brand':{info.get('brand', '')}\\n 'categories':{info.get('categories', '')}\"\n",
    "    embedding = model.encode(semantics)\n",
    "    item_embeddings.append({'ItemID': itemID, 'embedding': embedding.tolist()})\n",
    "\n",
    "# Convert to DataFrame\n",
    "item_emb_df = pd.DataFrame(item_embeddings)\n",
    "\n",
    "print(\"\\nItem embeddings DataFrame shape:\", item_emb_df.shape)\n",
    "print(\"The first 3 rows of item embeddings DataFrame:\\n\", item_emb_df.head(3))\n",
    "\n",
    "# Save to parquet file\n",
    "item_emb_df.to_parquet(f'./{dataset_name}/item_emb.parquet', index=False)\n",
    "\n",
    "print(\"Item embeddings saved to item_emb.parquet.\")\n",
    "# embeddings = np.array([item['embedding'] for item in item_embeddings])\n",
    "# np.save(f'./{dataset_name}/item_emb.npy', embeddings)\n",
    "\n",
    "# print(\"Item embeddings saved to item_emb.npy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIGER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
