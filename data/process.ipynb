{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert raw data to 'strict' json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os\n",
    "dataset_name = \"Beauty\"\n",
    "os.makedirs(dataset_name, exist_ok=True)\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield json.dumps(eval(l))\n",
    "\n",
    "# Beauty dataset\n",
    "f = open(f\"./{dataset_name}/{dataset_name}.json\", 'w')\n",
    "for l in parse(f\"reviews_{dataset_name}_5.json.gz\"):\n",
    "  f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 198502\n",
      "First line: {\"reviewerID\": \"A1YJEY40YUW4SE\", \"asin\": \"7806397051\", \"reviewerName\": \"Andrea\", \"helpful\": [3, 4], \"reviewText\": \"Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.\", \"overall\": 1.0, \"summary\": \"Don't waste your money\", \"unixReviewTime\": 1391040000, \"reviewTime\": \"01 30, 2014\"}\n"
     ]
    }
   ],
   "source": [
    "# print the number of lines in the file and the first line\n",
    "data = open(f\"./{dataset_name}/{dataset_name}.json\", 'r')\n",
    "print(\"Number of lines:\", sum(1 for _ in data))\n",
    "data.seek(0)  # Reset file pointer to the beginning\n",
    "print(\"First line:\", data.readline().strip())\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_num: 22363\n",
      "the first five userID mapping: [('A1YJEY40YUW4SE', 1), ('A60XNB876KYML', 2), ('A3G6XNM240RMWA', 3), ('A1PQFP6SAJ6D80', 4), ('A38FVHZTNQ271F', 5)]\n",
      "item_num: 12101\n",
      "the first five itemID mapping: [('7806397051', 1), ('9759091062', 2), ('9788072216', 3), ('9790790961', 4), ('9790794231', 5)]\n",
      "user-item mapping: [(1, [6846, 7873, 4585, 1, 5406]), (2, [816, 10406, 11194, 11651, 9716, 1, 233]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204, 5863, 6609]), (4, [5522, 439, 5161, 11140, 1, 7849]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500, 11444, 11390])]\n",
      "training data: [(1, [6846, 7873, 4585]), (2, [816, 10406, 11194, 11651, 9716]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204]), (4, [5522, 439, 5161, 11140]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500])]\n",
      "validation data: [(1, [6846, 7873, 4585, 1]), (2, [816, 10406, 11194, 11651, 9716, 1]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204, 5863]), (4, [5522, 439, 5161, 11140, 1]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500, 11444])]\n",
      "testing data: [(1, [6846, 7873, 4585, 1, 5406]), (2, [816, 10406, 11194, 11651, 9716, 1, 233]), (3, [1, 6050, 7977, 5252, 4211, 243, 11204, 5863, 6609]), (4, [5522, 439, 5161, 11140, 1, 7849]), (5, [1, 10470, 10064, 9403, 10362, 4758, 6500, 11444, 11390])]\n",
      "\n",
      "Training data shape: (22363, 3)\n",
      "the first 3 rows of training data:\n",
      "    user                           history  target\n",
      "0     1                      [6846, 7873]    4585\n",
      "1     2        [816, 10406, 11194, 11651]    9716\n",
      "2     3  [1, 6050, 7977, 5252, 4211, 243]   11204\n",
      "\n",
      "Validation data shape: (22363, 3)\n",
      "the first 3 rows of validation data:\n",
      "    user                                  history  target\n",
      "0     1                       [6846, 7873, 4585]       1\n",
      "1     2         [816, 10406, 11194, 11651, 9716]       1\n",
      "2     3  [1, 6050, 7977, 5252, 4211, 243, 11204]    5863\n",
      "\n",
      "Testing data shape: (22363, 3)\n",
      "the first 3 rows of testing data:\n",
      "    user                                        history  target\n",
      "0     1                          [6846, 7873, 4585, 1]    5406\n",
      "1     2            [816, 10406, 11194, 11651, 9716, 1]     233\n",
      "2     3  [1, 6050, 7977, 5252, 4211, 243, 11204, 5863]    6609\n",
      "Data saved to parquet files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize mapping dictionaries\n",
    "userID_mapping = {}\n",
    "itemID_mapping = {}\n",
    "\n",
    "# Open the JSON file for reading\n",
    "data = open(f\"./{dataset_name}/{dataset_name}.json\", 'r')\n",
    "\n",
    "# Initialize lists to store userID, itemID, and timestamp\n",
    "userIDs = []\n",
    "itemIDs = []\n",
    "timestamps = []\n",
    "\n",
    "# Process each line in the JSON file\n",
    "for line in data:\n",
    "    review = json.loads(line.strip())\n",
    "    userID = review['reviewerID']\n",
    "    itemID = review['asin']\n",
    "    timestamp = review['unixReviewTime']\n",
    "    \n",
    "    # Map userID to an integer starting from 1\n",
    "    if userID not in userID_mapping:\n",
    "        userID_mapping[userID] = len(userID_mapping) + 1\n",
    "    \n",
    "    # Map itemID to an integer starting from 1\n",
    "    if itemID not in itemID_mapping:\n",
    "        itemID_mapping[itemID] = len(itemID_mapping) + 1\n",
    "    \n",
    "    # Append mapped values and timestamp to lists\n",
    "    userIDs.append(userID_mapping[userID])\n",
    "    itemIDs.append(itemID_mapping[itemID])\n",
    "    timestamps.append(timestamp)\n",
    "\n",
    "# Save mapping dictionaries as .npy files\n",
    "np.save(f'./{dataset_name}/user_mapping.npy', userID_mapping)\n",
    "print(\"user_num:\", len(userID_mapping))\n",
    "print(\"the first five userID mapping:\", list(userID_mapping.items())[:5])\n",
    "np.save(f'./{dataset_name}/item_mapping.npy', itemID_mapping)\n",
    "print(\"item_num:\", len(itemID_mapping))\n",
    "print(\"the first five itemID mapping:\", list(itemID_mapping.items())[:5])\n",
    "\n",
    "# Group itemIDs by userID and sort by timestamp\n",
    "user_item_mapping = {}\n",
    "for userID, itemID, timestamp in zip(userIDs, itemIDs, timestamps):\n",
    "    if userID not in user_item_mapping:\n",
    "        user_item_mapping[userID] = []\n",
    "    user_item_mapping[userID].append((itemID, timestamp))\n",
    "\n",
    "# Sort itemIDs for each user by timestamp\n",
    "for userID in user_item_mapping:\n",
    "    user_item_mapping[userID].sort(key=lambda x: x[1])\n",
    "    user_item_mapping[userID] = [item[0] for item in user_item_mapping[userID]]\n",
    "\n",
    "# Print a sample of the results\n",
    "print(\"user-item mapping:\", list(user_item_mapping.items())[:5])\n",
    "\n",
    "# Split data into training, validation, and testing sets using leave-one-out strategy\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for userID, item_sequence in user_item_mapping.items():\n",
    "    # Assign the last item for testing, the second-to-last for validation, and the rest for training\n",
    "    train_data[userID] = item_sequence[:-2]\n",
    "    val_data[userID] = item_sequence[:-1]\n",
    "    test_data[userID] = item_sequence\n",
    "\n",
    "# Print a sample of the split data\n",
    "print(\"training data:\", list(train_data.items())[:5])\n",
    "print(\"validation data:\", list(val_data.items())[:5])\n",
    "print(\"testing data:\", list(test_data.items())[:5])\n",
    "\n",
    "# Prepare data for train, validation, and test sets\n",
    "def prepare_data(data_dict):\n",
    "    rows = []\n",
    "    for userID, item_sequence in data_dict.items():\n",
    "        history = item_sequence[:-1]\n",
    "        target = item_sequence[-1]\n",
    "        rows.append({'user': userID, 'history': history, 'target': target})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create dataframes for train, validation, and test sets\n",
    "train_df = prepare_data(train_data)\n",
    "print(\"\\nTraining data shape:\", train_df.shape)\n",
    "print(\"the first 3 rows of training data:\\n\", train_df.head(3))\n",
    "val_df = prepare_data(val_data)\n",
    "print(\"\\nValidation data shape:\", val_df.shape)\n",
    "print(\"the first 3 rows of validation data:\\n\", val_df.head(3))\n",
    "test_df = prepare_data(test_data)\n",
    "print(\"\\nTesting data shape:\", test_df.shape)\n",
    "print(\"the first 3 rows of testing data:\\n\", test_df.head(3))\n",
    "\n",
    "# Save dataframes to parquet files\n",
    "train_df.to_parquet(f'./{dataset_name}/train.parquet', index=False)\n",
    "val_df.to_parquet(f'./{dataset_name}/valid.parquet', index=False)\n",
    "test_df.to_parquet(f'./{dataset_name}/test.parquet', index=False)\n",
    "\n",
    "print(\"Data saved to parquet files.\")\n",
    "\n",
    "data.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Item Semantic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty metadata \n",
    "f = open(f\"./{dataset_name}/{dataset_name}_metadata.json\", 'w')\n",
    "for l in parse(f\"meta_{dataset_name}.json.gz\"):\n",
    "  f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemID: 1, Info: {'title': 'WAWO 15 Color Professionl Makeup Eyeshadow Camouflage Facial Concealer Neutral Palette', 'price': 5.04, 'salesRank': {'Beauty': 10486}, 'brand': 'COKA', 'categories': [['Beauty', 'Makeup', 'Face', 'Concealers & Neutralizers']]}\n",
      "ItemID: 2, Info: {'title': 'Xtreme Brite Brightening Gel 1oz.', 'price': 19.99, 'salesRank': {'Beauty': 52254}, 'brand': 'Xtreme Brite', 'categories': [['Beauty', 'Hair Care', 'Styling Products', 'Creams, Gels & Lotions']]}\n",
      "ItemID: 3, Info: {'title': 'Prada Candy By Prada Eau De Parfum Spray 1.7 Oz For Women', 'price': 65.86, 'salesRank': {'Beauty': 78916}, 'brand': 'Prada', 'categories': [['Beauty', 'Fragrance', \"Women's\", 'Eau de Parfum']]}\n",
      "ItemID: 4, Info: {'title': 'Versace Bright Crystal Eau de Toilette Spray for Women, 3 Ounce', 'price': 52.33, 'salesRank': {'Beauty': 764}, 'brand': 'Versace', 'categories': [['Beauty', 'Fragrance', \"Women's\", 'Eau de Toilette']]}\n",
      "ItemID: 5, Info: {'title': 'Stella McCartney Stella', 'price': None, 'salesRank': {'Beauty': 142503}, 'brand': None, 'categories': [['Beauty', 'Fragrance', \"Women's\", 'Eau de Parfum']]}\n"
     ]
    }
   ],
   "source": [
    "# Open the metadata file for reading\n",
    "with open(f\"./{dataset_name}/{dataset_name}_metadata.json\", 'r') as metadata_file:\n",
    "    # Create a reverse mapping from itemID to asin\n",
    "    reverse_itemID_mapping = {v: k for k, v in itemID_mapping.items()}\n",
    "    \n",
    "    # Initialize a dictionary to store the extracted information\n",
    "    item_info = {}\n",
    "    \n",
    "    # Process each line in the metadata file\n",
    "    for line in metadata_file:\n",
    "        metadata = json.loads(line.strip())\n",
    "        asin = metadata.get('asin')\n",
    "        \n",
    "        # Check if the asin exists in the reverse mapping\n",
    "        if asin in reverse_itemID_mapping.values():\n",
    "            itemID = itemID_mapping[asin]\n",
    "            item_info[itemID] = {\n",
    "                'title': metadata.get('title') if metadata.get('title') else None,\n",
    "                'price': metadata.get('price') if metadata.get('price') else None,\n",
    "                'salesRank': metadata.get('salesRank') if metadata.get('salesRank') else None,\n",
    "                'brand': metadata.get('brand') if metadata.get('brand') else None,\n",
    "                'categories': metadata.get('categories') if metadata.get('categories') else None,\n",
    "            }\n",
    "\n",
    "# Print the information for the first 5 items\n",
    "for itemID, info in list(item_info.items())[:5]:\n",
    "    print(f\"ItemID: {itemID}, Info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item embeddings DataFrame shape: (12101, 2)\n",
      "The first 3 rows of item embeddings DataFrame:\n",
      "    ItemID                                          embedding\n",
      "0       1  [0.005812718532979488, 0.0014312762068584561, ...\n",
      "1       2  [-0.005331065971404314, -0.0387347936630249, 0...\n",
      "2       3  [-0.03325002267956734, -0.030644146725535393, ...\n",
      "Item embeddings saved to item_emb.parquet.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "#  modelscope download --model sentence-transformers/sentence-t5-base  --local_dir ./dir\n",
    "model = SentenceTransformer('./sentence-t5-base')\n",
    "\n",
    "# Prepare data for embedding\n",
    "item_embeddings = []\n",
    "for itemID, info in item_info.items():\n",
    "    # Combine relevant fields into a single text for embedding\n",
    "    semantics = f\"'title':{info.get('title', '')}\\n 'price':{info.get('price', '')}\\n 'salesRank':{info.get('salesRank', '')}\\n 'brand':{info.get('brand', '')}\\n 'categories':{info.get('categories', '')}\"\n",
    "    embedding = model.encode(semantics)\n",
    "    item_embeddings.append({'ItemID': itemID, 'embedding': embedding.tolist()})\n",
    "\n",
    "# Convert to DataFrame\n",
    "item_emb_df = pd.DataFrame(item_embeddings)\n",
    "\n",
    "print(\"\\nItem embeddings DataFrame shape:\", item_emb_df.shape)\n",
    "print(\"The first 3 rows of item embeddings DataFrame:\\n\", item_emb_df.head(3))\n",
    "\n",
    "# Save to parquet file\n",
    "item_emb_df.to_parquet(f'./{dataset_name}/item_emb.parquet', index=False)\n",
    "\n",
    "print(\"Item embeddings saved to item_emb.parquet.\")\n",
    "# embeddings = np.array([item['embedding'] for item in item_embeddings])\n",
    "# np.save(f'./{dataset_name}/item_emb.npy', embeddings)\n",
    "\n",
    "# print(\"Item embeddings saved to item_emb.npy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIGER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
